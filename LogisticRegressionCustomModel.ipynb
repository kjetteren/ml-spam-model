{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'dataset/PhishingDataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select features and label\n",
    "LABEL = df.iloc[:, -1:].columns[0]\n",
    "cols = [\n",
    "    'LineOfCode', 'NoOfExternalRef', 'LargestLineLength', 'URLLength', 'NoOfImage', 'NoOfJS', 'NoOfSelfRef', 'NoOfCSS',\n",
    "    'URLCharProb', 'CharContinuationRate', 'LetterRatioInURL', 'IsHTTPS', 'SpacialCharRatioInURL', 'NoOfEmptyRef',\n",
    "    'NoOfOtherSpecialCharsInURL', 'HasDescription', 'HasSocialNet', 'DomainLength', 'DegitRatioInURL', 'NoOfDegitsInURL',\n",
    "    'HasCopyrightInfo', 'NoOfLettersInURL', 'TLDLegitimateProb', 'DomainTitleMatchScore', 'IsResponsive',\n",
    "    'HasHiddenFields', 'HasSubmitButton', 'NoOfSubDomain', 'HasFavicon', 'HasTitle'\n",
    "]\n",
    "\n",
    "X = df[cols].values  # Features\n",
    "y = df[LABEL].values  # Target label\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize logistic regression model\n",
    "model = LogisticRegression(solver='liblinear')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")"
   ],
   "id": "e4cb39c3c1e45bca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Based on the results change the code to use the best parameters",
   "id": "9e9bf7a44528ffe9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    'C': [0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")"
   ],
   "id": "c814c9b6bafaa53c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next iteration",
   "id": "808585ce0cf6c13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    'C': [0.145, 0.1475, 0.15, 0.1525, 0.155],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")"
   ],
   "id": "dccb8010c754d3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Best Parameters: {'C': 0.15, 'penalty': 'l1'}\n",
    "Training model with best parameters"
   ],
   "id": "eedba03ae2b3bcf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ],
   "id": "7217220509658763",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Confusion Matrix",
   "id": "8cb04c6f1eb56a15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "CM = pd.DataFrame(columns=['Mode', 'TP', 'TN', 'FP', 'FN'])\n",
    "\n",
    "# Add the values to the DataFrame\n",
    "newRow = {\n",
    "    'Mode': \"LogisticRegression\",\n",
    "    'TP': TP,\n",
    "    'TN': TN,\n",
    "    'FP': FP,\n",
    "    'FN': FN\n",
    "}\n",
    "CM = pd.concat([CM, pd.DataFrame([newRow])], ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(CM)"
   ],
   "id": "a0235cc02e0f5890",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
